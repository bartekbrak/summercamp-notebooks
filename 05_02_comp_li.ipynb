{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational linguistics\n",
    "====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first. Let's style our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('css/pawel.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required libraries\n",
    "-------------------------\n",
    "\n",
    "1. Install nltk - ```pip install nltk```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and querying a simple language model\n",
    "-----------------------------\n",
    "\n",
    "A language model is simply put statistical information about the language. \n",
    "In its most simple form it is just word occurence statistics.\n",
    "\n",
    "There are many types of language models. Most common are based on words. But there are also character based models:\n",
    "\n",
    "Here is a demo of a system trained on characters\n",
    "\n",
    "http://www.cs.toronto.edu/~ilya/rnn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful vocabulary\n",
    "--------------------------\n",
    "\n",
    "**Corpus** - a collection of text - normally from 1 domain\n",
    "\n",
    "**Parallel corpus** - a collection of source text and translated text - very useful for translation purposes\n",
    "\n",
    "**Tokenization** - a process of splitting (eg. sentences, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download corpus for analysis\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://norvig.com/big.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read text and clean it (slightly)\n",
    "--------------------\n",
    "\n",
    "Open the corpus and do some cleaning\n",
    "\n",
    "**Exercises:**\n",
    "1. Read the contents of the big.txt file into a variable\n",
    "2. Replace all \\n with a single space\n",
    "3. Convert all text to lowercase\n",
    "4. Print out the beginning of the text 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class=\"spoiler\">\n",
    "\n",
    "text = open(\"big.txt\").read()\n",
    "text = text.replace(\"\\n\",\" \").lower()\n",
    "print text[:500]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting\n",
    "-------------------------\n",
    "\n",
    "It is useful to split the text into sentences.\n",
    "\n",
    "Questions:\n",
    "1. Why a naive method like splitting with \".\" won't work very well?\n",
    "\n",
    "There exists libraries for sentence tokenization which are smarter than that.\n",
    "Let's import sentence tokenizer from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences[:10]:\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word splitting (tokenization)\n",
    "-----------------\n",
    "\n",
    "Word tokenization is as important or maybe even more than sentence splitting?\n",
    "\n",
    "Questions:\n",
    "1. Why a naive method of splitting with blank characters won't work very well?\n",
    "\n",
    "There exist tools for this also. Let's import word tokenization function from nltk.\n",
    "In our company we use a custom tokenizer - because our domain is very specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print word_tokenize(\"Hello, world.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "    \n",
    "1. Tokenize each sentence into words (tip: use map function)\n",
    "2. Print out first 10 sentences tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class=\"spoiler\">\n",
    "\n",
    "sentences_tokenized = map(word_tokenize, sentences)\n",
    "\n",
    "for sentence in sentences_tokenized[:10]:\n",
    "    print sentence\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams\n",
    "---------------------\n",
    "\n",
    "From wikipedia (https://en.wikipedia.org/wiki/N-gram)\n",
    "\n",
    "_In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.[1]_\n",
    "\n",
    "- 1-gram (aka unigram) - 1 word\n",
    "- 2-gram (aka bigram) - 2 consecutive words\n",
    "- 3-gram (aka trigram) - 3 consecutive words\n",
    "- ...\n",
    "- n-gram - n consecutive words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "----------------\n",
    "\n",
    "In a text:\n",
    "\n",
    "    A simple complete sentence consists of a single clause\n",
    "    \n",
    "Questions:\n",
    "1. How many 1-grams, 2-grams and n-grams are in the sentence? Enumerate them all\n",
    "2. Create a function ```ngrams``` that given a list of words an n returns a list of n-grams\n",
    "3. Print out all n-grams from our sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"A simple complete sentence consists of a single clause\"\n",
    "words = sentence.split()\n",
    "\n",
    "# put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class=\"spoiler\">\n",
    "\n",
    "def ngrams(words, n):\n",
    "    for i in range(len(words)-n+1):\n",
    "        yield tuple(words[i:(i+n)])\n",
    "            \n",
    "all_ngrams = 0\n",
    "for n in range(1,len(words)+1):\n",
    "    print \"{}-gram\".format(n)\n",
    "    for ngram in ngrams(words, n):\n",
    "        print ngram\n",
    "        all_ngrams += 1\n",
    "    print\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a simple language model\n",
    "-------------------\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Based on our corpus - create a list of all 1,2,3,4,5-grams (Not necessarily unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class=\"spoiler\">\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "ngrams_all = []\n",
    "for n in range(1,6):\n",
    "    print n\n",
    "    for sentence in sentences_tokenized:\n",
    "        for ngram in ngrams(sentence,n):\n",
    "            ngrams_all.append(ngram)\n",
    "\n",
    "ngrams_freq = Counter(ngrams_all)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Print out the most frequent 1,2,3,4,5-grams (eg top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class=\"spoiler\">\n",
    "\n",
    "for n in range(1,6):\n",
    "    print \"most frequent {}-grams\".format(n)\n",
    "    ngrams_freq_ = filter(lambda ngram: len(ngram[0]) == n, ngrams_freq.most_common())\n",
    "    for ngram,freq in ngrams_freq_[:10]:\n",
    "        print ngram, freq\n",
    "    print\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun time - generating random sentences from the bigram model\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigrams = filter(lambda ngram: len(ngram[0]) == 1, ngrams_freq.most_common())\n",
    "bigrams = filter(lambda ngram: len(ngram[0]) == 2, ngrams_freq.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "from random import random\n",
    "from copy import copy\n",
    "\n",
    "class WeightedSampler():\n",
    "    def __init__(self, freqs):\n",
    "        # copy the counter dictionary\n",
    "        self.freqs = copy(freqs)\n",
    "        self.normalize_probabilities()\n",
    "        self.calculate_cdf()\n",
    "        \n",
    "    def normalize_probabilities(self):\n",
    "        s = float(sum([f for k,f in self.freqs]))\n",
    "        for i in range(len(self.freqs)):\n",
    "            self.freqs[i] = (self.freqs[i][0], self.freqs[i][1] / s)\n",
    "            \n",
    "    def calculate_cdf(self):\n",
    "        self.cdf = [self.freqs[0][1]]\n",
    "        for k,prob in self.freqs[1:]:\n",
    "            self.cdf.append(self.cdf[-1] + prob)\n",
    "            \n",
    "    def random_choice(self):\n",
    "        return self.freqs[bisect(self.cdf,random())][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_sentence = \"hi\".split()\n",
    "\n",
    "while True:\n",
    "    bigrams_ = filter(lambda t: t[0][0] == random_sentence[-1], bigrams)\n",
    "    bigrams_sampler = WeightedSampler(bigrams_)\n",
    "    continuation = bigrams_sampler.random_choice()\n",
    "    random_sentence.append(continuation[1])\n",
    "    print \" \".join(random_sentence)\n",
    "    if len(random_sentence) > 15:\n",
    "        break\n",
    "    print "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
